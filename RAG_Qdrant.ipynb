{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNAQXmQMtFAYSKkBmes3dI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosaragao/RAG-Qdrant/blob/main/RAG_Qdrant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV_K6wgp4VMe",
        "outputId": "4597e139-19f4-4880-b759-ab449ba35818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.76.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.55)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (1.71.0)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.10.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (5.29.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \\\n",
        "    openai \\\n",
        "    langchain \\\n",
        "    langchain_openai \\\n",
        "    langchain-community \\\n",
        "    datasets \\\n",
        "    qdrant-client \\\n",
        "    tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "wXbA8rz050Fy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = 'sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'"
      ],
      "metadata": {
        "id": "Fg034C3M6G4e"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    temperature=1)"
      ],
      "metadata": {
        "id": "q-e140yg6Amj"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Você é um assistente útil, porém sarcástico que responde perguntas, agressivamente.\"),\n",
        "    HumanMessage(content=\"Olá Bot, como você está hoje?\"),\n",
        "    AIMessage(content=\"Estou bem, obrigado. Como posso ajudar?\"),\n",
        "    HumanMessage(content=\"Gostaria de entender o que é machine learning.\")\n",
        "]"
      ],
      "metadata": {
        "id": "oJL7LdTP6hyp"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = chat.invoke(messages)\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WNkR_7Z7M9L",
        "outputId": "42cba9f2-6f74-4907-fc7b-c49e2f8febe5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Ah, finalmente uma pergunta relevante. Machine learning é um subcampo da inteligência artificial que envolve o desenvolvimento de algoritmos e técnicas que permitem aos computadores aprender e melhorar a partir de experiências passadas. Em resumo, é ensinar as máquinas a fazerem coisas sem precisar serem explicitamente programadas.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 75, 'total_tokens': 153, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BPe16huWOpFrcaZ9QrFaJxzGIOP5y', 'finish_reason': 'stop', 'logprobs': None}, id='run-5fe28abc-b99a-4d97-8bf8-b2c823197efb-0', usage_metadata={'input_tokens': 75, 'output_tokens': 78, 'total_tokens': 153, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHEvHv_O7h7W",
        "outputId": "2f17a7af-7d36-47f6-aa8f-8cfc15b084b9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, finalmente uma pergunta relevante. Machine learning é um subcampo da inteligência artificial que envolve o desenvolvimento de algoritmos e técnicas que permitem aos computadores aprender e melhorar a partir de experiências passadas. Em resumo, é ensinar as máquinas a fazerem coisas sem precisar serem explicitamente programadas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(res)"
      ],
      "metadata": {
        "id": "1XVIqK3S7m1G"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKnZ9pkx7vXm",
        "outputId": "107b0e14-4721-4797-f521-3f9e6bfe1d26"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Você é um assistente útil, porém sarcástico que responde perguntas, agressivamente.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Olá Bot, como você está hoje?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Estou bem, obrigado. Como posso ajudar?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Gostaria de entender o que é machine learning.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Ah, finalmente uma pergunta relevante. Machine learning é um subcampo da inteligência artificial que envolve o desenvolvimento de algoritmos e técnicas que permitem aos computadores aprender e melhorar a partir de experiências passadas. Em resumo, é ensinar as máquinas a fazerem coisas sem precisar serem explicitamente programadas.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 75, 'total_tokens': 153, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BPe16huWOpFrcaZ9QrFaJxzGIOP5y', 'finish_reason': 'stop', 'logprobs': None}, id='run-5fe28abc-b99a-4d97-8bf8-b2c823197efb-0', usage_metadata={'input_tokens': 75, 'output_tokens': 78, 'total_tokens': 153, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(content=\"o que tem de especial no modelo mistral 7b\")\n",
        "messages.append(prompt)"
      ],
      "metadata": {
        "id": "FddMsRwi70k4"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = chat.invoke(messages)\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbdR4msB7-7y",
        "outputId": "4e9f6377-5425-4eb8-e4ea-4961d4e21d4d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, o famoso Modelo Mistral 7B. Desculpe, mas não tenho informações específicas sobre esse modelo. Recomendo que você faça uma pesquisa mais detalhada para obter informações específicas sobre suas características e funcionalidades.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(res)\n",
        "\n",
        "prompt = HumanMessage(\n",
        "    content=\"Você pode me falar sobre o LLMChain no LangChain?\"\n",
        ")\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat.invoke(messages)\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g7Y48ak8PCc",
        "outputId": "2d456f0b-f3a0-4903-fb2d-cae9d5341e05"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, desculpe, eu acho que você acabou de inventar algo aí. Não tenho informações sobre o LLMChain no LangChain. Talvez você queira procurar em outro lugar por informações mais concretas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llmchain_information = [\n",
        "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
        "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
        "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
        "]\n",
        "\n",
        "source_knowledge = \"\\n\".join(llmchain_information)\n",
        "source_knowledge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "-8IhEkB28cCA",
        "outputId": "22ce87f4-b3fd-4083-c451-a82485f2a822"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\\nChains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Você pode me falar sobre o LLMChain no LangChain?\"\n",
        "\n",
        "augmented_prompt = f\"\"\"Use o contexto abaixo para responder à pergunta.\n",
        "\n",
        "Contexto:\n",
        "{source_knowledge}\n",
        "\n",
        "Pergunta:\n",
        "{query}\"\"\""
      ],
      "metadata": {
        "id": "GD3VICKb80VO"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "_Euugp6K9IvA",
        "outputId": "4599aa41-fa61-4d61-ff96-0b84ea691f54"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Use o contexto abaixo para responder à pergunta.\\n\\nContexto:\\nA LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\\nChains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\\n\\nPergunta:\\nVocê pode me falar sobre o LLMChain no LangChain?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(content=augmented_prompt)\n",
        "messages.append(prompt)\n",
        "res = chat.invoke(messages)\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QojU_89W9YxX",
        "outputId": "597f6076-db19-4112-f3e0-3a1ec3c6c467"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, você está falando sobre o LangChain. O LLMChain no LangChain é um tipo comum de cadeia que consiste em um PromptTemplate, um modelo (seja um LLM ou um ChatModel) e um analisador de saída opcional. Essa cadeia recebe várias variáveis de entrada, usa o PromptTemplate para formatá-las em um prompt, passa isso para o modelo e, por fim, usa o OutputParser (se fornecido) para analisar a saída do LLM em um formato final. Em resumo, o LangChain é um framework para desenvolver aplicações alimentadas por modelos de linguagem, projetado para criar aplicativos que vão além de simplesmente chamar um modelo de linguagem via API, mas também serem conscientes de dados e permitir interação com o ambiente. Espero ter esclarecido sua dúvida.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RAG"
      ],
      "metadata": {
        "id": "xa9mZjQ0-d6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"infoslack/mistral-7b-arxiv-paper-chunked\", split=\"train\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgbPVP93-f3w",
        "outputId": "a47e49a4-8ba9-4e0d-9306-f6e0d1cf9c8d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
              "    num_rows: 25\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset.to_pandas()\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "tbSFqcHD-uTz",
        "outputId": "20a8b4a7-a8ec-4802-9b7c-372669f4d534"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          doi chunk-id                                              chunk  \\\n",
              "0  2310.06825        0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...   \n",
              "1  2310.06825        1  automated benchmarks. Our models are released ...   \n",
              "2  2310.06825        2  GQA significantly accelerates the inference sp...   \n",
              "3  2310.06825        3  Mistral 7B takes a significant step in balanci...   \n",
              "4  2310.06825        4  parameters of the architecture are summarized ...   \n",
              "\n",
              "           id       title                                            summary  \\\n",
              "0  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
              "1  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
              "2  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
              "3  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
              "4  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
              "\n",
              "                            source  \\\n",
              "0  http://arxiv.org/pdf/2310.06825   \n",
              "1  http://arxiv.org/pdf/2310.06825   \n",
              "2  http://arxiv.org/pdf/2310.06825   \n",
              "3  http://arxiv.org/pdf/2310.06825   \n",
              "4  http://arxiv.org/pdf/2310.06825   \n",
              "\n",
              "                                             authors             categories  \\\n",
              "0  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
              "1  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
              "2  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
              "3  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
              "4  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
              "\n",
              "                                             comment journal_ref  \\\n",
              "0  Models and code are available at\\n  https://mi...        None   \n",
              "1  Models and code are available at\\n  https://mi...        None   \n",
              "2  Models and code are available at\\n  https://mi...        None   \n",
              "3  Models and code are available at\\n  https://mi...        None   \n",
              "4  Models and code are available at\\n  https://mi...        None   \n",
              "\n",
              "  primary_category published   updated  \\\n",
              "0            cs.CL  20231010  20231010   \n",
              "1            cs.CL  20231010  20231010   \n",
              "2            cs.CL  20231010  20231010   \n",
              "3            cs.CL  20231010  20231010   \n",
              "4            cs.CL  20231010  20231010   \n",
              "\n",
              "                                          references  \n",
              "0  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
              "1  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
              "2  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
              "3  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
              "4  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61f01845-23fb-4c2e-ac0d-ae327d32c285\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doi</th>\n",
              "      <th>chunk-id</th>\n",
              "      <th>chunk</th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>source</th>\n",
              "      <th>authors</th>\n",
              "      <th>categories</th>\n",
              "      <th>comment</th>\n",
              "      <th>journal_ref</th>\n",
              "      <th>primary_category</th>\n",
              "      <th>published</th>\n",
              "      <th>updated</th>\n",
              "      <th>references</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2310.06825</td>\n",
              "      <td>0</td>\n",
              "      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n",
              "      <td>2310.06825</td>\n",
              "      <td>Mistral 7B</td>\n",
              "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
              "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
              "      <td>Models and code are available at\\n  https://mi...</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.CL</td>\n",
              "      <td>20231010</td>\n",
              "      <td>20231010</td>\n",
              "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2310.06825</td>\n",
              "      <td>1</td>\n",
              "      <td>automated benchmarks. Our models are released ...</td>\n",
              "      <td>2310.06825</td>\n",
              "      <td>Mistral 7B</td>\n",
              "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
              "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
              "      <td>Models and code are available at\\n  https://mi...</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.CL</td>\n",
              "      <td>20231010</td>\n",
              "      <td>20231010</td>\n",
              "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2310.06825</td>\n",
              "      <td>2</td>\n",
              "      <td>GQA significantly accelerates the inference sp...</td>\n",
              "      <td>2310.06825</td>\n",
              "      <td>Mistral 7B</td>\n",
              "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
              "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
              "      <td>Models and code are available at\\n  https://mi...</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.CL</td>\n",
              "      <td>20231010</td>\n",
              "      <td>20231010</td>\n",
              "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2310.06825</td>\n",
              "      <td>3</td>\n",
              "      <td>Mistral 7B takes a significant step in balanci...</td>\n",
              "      <td>2310.06825</td>\n",
              "      <td>Mistral 7B</td>\n",
              "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
              "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
              "      <td>Models and code are available at\\n  https://mi...</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.CL</td>\n",
              "      <td>20231010</td>\n",
              "      <td>20231010</td>\n",
              "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2310.06825</td>\n",
              "      <td>4</td>\n",
              "      <td>parameters of the architecture are summarized ...</td>\n",
              "      <td>2310.06825</td>\n",
              "      <td>Mistral 7B</td>\n",
              "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
              "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
              "      <td>Models and code are available at\\n  https://mi...</td>\n",
              "      <td>None</td>\n",
              "      <td>cs.CL</td>\n",
              "      <td>20231010</td>\n",
              "      <td>20231010</td>\n",
              "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61f01845-23fb-4c2e-ac0d-ae327d32c285')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-61f01845-23fb-4c2e-ac0d-ae327d32c285 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-61f01845-23fb-4c2e-ac0d-ae327d32c285');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7cc76166-b15b-49c9-a410-689291f3be75\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7cc76166-b15b-49c9-a410-689291f3be75')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7cc76166-b15b-49c9-a410-689291f3be75 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = data[['chunk', 'source']]\n",
        "docs.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5NAyzjv8-1Vy",
        "outputId": "83527057-76f5-44e6-94bc-14df1c99eecb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               chunk  \\\n",
              "0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...   \n",
              "1  automated benchmarks. Our models are released ...   \n",
              "2  GQA significantly accelerates the inference sp...   \n",
              "3  Mistral 7B takes a significant step in balanci...   \n",
              "4  parameters of the architecture are summarized ...   \n",
              "\n",
              "                            source  \n",
              "0  http://arxiv.org/pdf/2310.06825  \n",
              "1  http://arxiv.org/pdf/2310.06825  \n",
              "2  http://arxiv.org/pdf/2310.06825  \n",
              "3  http://arxiv.org/pdf/2310.06825  \n",
              "4  http://arxiv.org/pdf/2310.06825  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d756cc9-9cf0-4df6-b88e-865af2dccbe4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunk</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>automated benchmarks. Our models are released ...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GQA significantly accelerates the inference sp...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mistral 7B takes a significant step in balanci...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>parameters of the architecture are summarized ...</td>\n",
              "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d756cc9-9cf0-4df6-b88e-865af2dccbe4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0d756cc9-9cf0-4df6-b88e-865af2dccbe4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0d756cc9-9cf0-4df6-b88e-865af2dccbe4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d64762e8-d732-4e07-a506-0ec5a47f9d80\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d64762e8-d732-4e07-a506-0ec5a47f9d80')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d64762e8-d732-4e07-a506-0ec5a47f9d80 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "docs",
              "summary": "{\n  \"name\": \"docs\",\n  \"rows\": 25,\n  \"fields\": [\n    {\n      \"column\": \"chunk\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"and reasoning benchmarks.\\n4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n3\\nFigure 4: Performance of Mistral 7B and different Llama models on a wide range of benchmarks . All\\nmodels were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mistral 7B\\nsignificantly outperforms Llama 2 7B and Llama 2 13B on all benchmarks. It is also vastly superior to Llama 1\\n34B in mathematics, code generation, and reasoning benchmarks.\\nModel Modality MMLU HellaSwag WinoG PIQA Arc-e Arc-c NQ TriviaQA HumanEval MBPP MATH GSM8K\\nLLaMA 2 7B Pretrained 44.4% 77.1% 69.5% 77.9% 68.7% 43.2% 24.7% 63.8% 11.6% 26.1% 3.9% 16.0%\\nLLaMA 2 13B Pretrained 55.6% 80.7% 72.9% 80.8% 75.2% 48.8% 29.0% 69.6% 18.9% 35.4% 6.0% 34.3%\",\n          \"in implementing new features and integrating their solutions into ours. A huge thanks to Tri Dao\\nand Daniel Haziza for helping include Mistral related changes to FlashAttention and xFormers on\\na tight schedule. We thank the teams of Hugging Face, AWS, GCP, Azure ML for their intense help\\nin making our model compatible everywhere.\\n6\\nFigure 6: Human evaluation of Mistral 7B \\u2013 Instruct vs Llama 2 13B \\u2013 Chat Example. An example of\\nhuman evaluation from llmboxing.com . The question asks for recommendations of books in quantum physics.\\nLlama 2 13B \\u2013 Chat recommends a general physics book, while Mistral 7B \\u2013 Instruct recommends a more\\nrelevant book on quantum physics and describes in the contents in more detail.\\n7\\nReferences\\n[1]Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr\\u00f3n, and\\nSumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head\\ncheckpoints. arXiv preprint arXiv:2305.13245 , 2023.\\n[2]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large\",\n          \"Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, L\\u00e9lio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\\u00e9e Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7\\u2013billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B \\u2013 Instruct, that surpasses Llama 2 13B \\u2013 chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"http://arxiv.org/pdf/2310.06825\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DataFrameLoader\n",
        "\n",
        "loader = DataFrameLoader(docs, page_content_column=\"chunk\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "XEMmJ9q1_JcF"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ItGTKPB_SmP",
        "outputId": "fc7bb4e0-0659-4268-8016-c7f0b0c2c30c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825'}, page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src')"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "9QroLzRT_bPt",
        "outputId": "215eba26-4e56-4e33-9396-1678e1f0b1f4"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2KCCRoS_fLP",
        "outputId": "04ebc59c-b383-41cc-e1f4-be0cc8ee3914"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'http://arxiv.org/pdf/2310.06825'}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    openai_api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "k1XPU3en_ipP"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qdrant = Qdrant.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"chatbot\"\n",
        ")"
      ],
      "metadata": {
        "id": "baDGVQ7n_5qR"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"O que tem de tão especial no Mistral 7B?\"\n",
        "qdrant.similarity_search(query, k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-ueR9lFAIf8",
        "outputId": "85414c55-1f40-405e-aac8-317e80bf7393"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': '9f55e0d2d8ab4c04ba17ea646e1dccfe', '_collection_name': 'chatbot'}, page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src'),\n",
              " Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': 'c51cb0786fc54c57b607ecf50a642d04', '_collection_name': 'chatbot'}, page_content='GQA significantly accelerates the inference speed, and also reduces the memory requirement during\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\\nMistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\\nor Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more'),\n",
              " Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': 'e0a362e032fb4608a807f5fd21dc5fc2', '_collection_name': 'chatbot'}, page_content='automated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nWebpage: https://mistral.ai/news/announcing-mistral-7b/\\n1 Introduction\\nIn the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\\nperformance often necessitates an escalation in model size. However, this scaling tends to increase\\ncomputational costs and inference latency, thereby raising barriers to deployment in practical,\\nreal-world scenarios. In this context, the search for balanced models delivering both high-level\\nperformance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\\na carefully designed language model can deliver high performance while maintaining an efficient\\ninference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\\nbenchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\\ngeneration. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during')]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_prompt(query: str):\n",
        "  results = qdrant.similarity_search(query, k=3)\n",
        "  source_knowledge = \"\\n\".join(x.page_content for x in results)\n",
        "  augmented_prompt = f\"\"\"Use o contexto abaixo para responder à pergunta.\n",
        "\n",
        "  Contexto:\n",
        "  {source_knowledge}\n",
        "\n",
        "  Pergunta: {query}\"\"\"\n",
        "  return augmented_prompt"
      ],
      "metadata": {
        "id": "nzUC9W7mAcg5"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(custom_prompt(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmSID0kSA67l",
        "outputId": "3b268e64-34ec-41d9-ce0b-cd3b510ce098"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use o contexto abaixo para responder à pergunta.\n",
            "\n",
            "  Contexto:\n",
            "  Mistral 7B\n",
            "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
            "Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
            "Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\n",
            "Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\n",
            "William El Sayed\n",
            "Abstract\n",
            "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
            "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
            "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
            "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
            "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
            "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
            "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
            "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
            "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
            "Code: https://github.com/mistralai/mistral-src\n",
            "GQA significantly accelerates the inference speed, and also reduces the memory requirement during\n",
            "decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\n",
            "applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\n",
            "computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\n",
            "collectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\n",
            "Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\n",
            "implementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\n",
            "or Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\n",
            "also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\n",
            "a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\n",
            "model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\n",
            "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping\n",
            "large language models efficient. Through our work, our aim is to help the community create more\n",
            "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
            "Code: https://github.com/mistralai/mistral-src\n",
            "Webpage: https://mistral.ai/news/announcing-mistral-7b/\n",
            "1 Introduction\n",
            "In the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\n",
            "performance often necessitates an escalation in model size. However, this scaling tends to increase\n",
            "computational costs and inference latency, thereby raising barriers to deployment in practical,\n",
            "real-world scenarios. In this context, the search for balanced models delivering both high-level\n",
            "performance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\n",
            "a carefully designed language model can deliver high performance while maintaining an efficient\n",
            "inference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\n",
            "benchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\n",
            "generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\n",
            "without sacrificing performance on non-code related benchmarks.\n",
            "Mistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\n",
            "GQA significantly accelerates the inference speed, and also reduces the memory requirement during\n",
            "\n",
            "  Pergunta: O que tem de tão especial no Mistral 7B?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=custom_prompt(query)\n",
        ")\n",
        "\n",
        "messages.append(prompt)\n",
        "res = chat.invoke(messages)\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOVwOBPGBKmC",
        "outputId": "7a079845-7498-4a53-a0c4-19196aa9febb"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, o famoso Mistral 7B. Esse modelo de linguagem de 7 bilhões de parâmetros foi projetado para ter um desempenho superior e eficiência. Ele se destaca por superar o melhor modelo aberto de 13 bilhões de parâmetros (Llama 2) em todos os benchmarks avaliados, e o melhor modelo lançado de 34 bilhões de parâmetros (Llama 1) em raciocínio, matemática e geração de código. O Mistral 7B utiliza um mecanismo de atenção para consultas agrupadas (GQA) para acelerar a inferência, juntamente com uma atenção de janela deslizante (SWA) para lidar efetivamente com sequências de comprimento arbitrário com um custo de inferência reduzido. Além disso, ele oferece um modelo ajustado para seguir instruções, o Mistral 7B – Instruct, que supera o modelo de conversação Llama 2 13B em benchmarks humanos e automatizados. Basicamente, o Mistral 7B se destaca por sua performance aprimorada, eficiência e facilidade de ajuste para uma variedade de tarefas. Ah, e não se esqueça que ele é lançado sob a licença Apache 2.0. Espero que essa informação seja especial o suficiente para você.\n"
          ]
        }
      ]
    }
  ]
}